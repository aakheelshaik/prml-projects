{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ec66516",
   "metadata": {},
   "source": [
    "## Decisions Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a57c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree(X, Y, num_nodes_stop=1, criterion='accuracy'):\n",
    "    \"\"\" Returns a decision tree trained on X and Y. \n",
    "    Stops splitting nodes when a node has hit a size of \"num_nodes_stop\" or lower.\n",
    "    Split criterion can be either 'accuracy' or 'entropy'.\n",
    "    Returns a tree (In whatever format that you find appropriate)\n",
    "    \"\"\"\n",
    "    tree ={}\n",
    "    init_val = -1\n",
    "    tree[0] = [init_val]*3\n",
    "    split_node_dec(tree, 0, X, Y, 0, num_nodes_stop, criterion)\n",
    "    return tree\n",
    "\n",
    "def eval_decision_tree(tree, test_X):\n",
    "    \"\"\" Takes in a tree, and a bunch of instances X and \n",
    "    returns the tree predicted values at those instances.\"\"\"\n",
    "    n = test_X.shape[0]\n",
    "    Y_pred = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        Y_pred[i] = pred_class(tree, 0, test_X[i])\n",
    "    return Y_pred\n",
    "    \n",
    "neg = 0\n",
    "pos = 0\n",
    "\n",
    "def split_node_dec(tree, node_pos, X, Y, depth, num_nodes_stop=1, criterion = 'accuracy', stop_depth = 1e7):\n",
    "    global neg, pos\n",
    "    n,d = X.shape\n",
    "    total_neg = np.sum((Y==-1))\n",
    "    total_pos = np.sum((Y==1))\n",
    "    one_class = False\n",
    "    if total_pos == 0 or total_pos == n:\n",
    "        one_class = True\n",
    "    if n <= num_nodes_stop or depth >= stop_depth or ((pos == total_pos) and (neg == total_neg)) or one_class:\n",
    "        if total_pos > total_neg:\n",
    "            class_pred = 1\n",
    "        else:\n",
    "            class_pred = -1\n",
    "        tree[node_pos] = [-1,-1, class_pred]\n",
    "        return\n",
    "\n",
    "    neg = total_neg\n",
    "    pos = total_pos\n",
    "    best_feat = 0\n",
    "    best_thresh = 0\n",
    "    best_acc = -np.inf\n",
    "    \n",
    "    for feat in range(d):\n",
    "        ids = X[:,feat].argsort()\n",
    "        X = X[ids]\n",
    "        Y = Y[ids]\n",
    "        min_feat = X[0][feat]\n",
    "        max_feat = X[-1][feat]\n",
    "        ### check here\n",
    "        num_pts  = 11\n",
    "        iter = max_feat - min_feat\n",
    "        iter = iter/num_pts\n",
    "        for pt in range(1, num_pts):\n",
    "            thresh = min_feat + pt*iter\n",
    "            pos_l = 0\n",
    "            left = 0\n",
    "            for sample in range(n):\n",
    "                if X[sample][feat] <= thresh:\n",
    "                    left+=1\n",
    "                    if Y[sample]>0:\n",
    "                        pos_l+=1\n",
    "            pos_r = total_pos - pos_l\n",
    "            right = n - left\n",
    "            dec = split_dec(criterion, pos_l, left, pos_r, right)\n",
    "            if dec >= best_acc:\n",
    "                best_acc = dec\n",
    "                best_feat = feat\n",
    "                best_thresh = thresh\n",
    "    X_left = X[X[:, best_feat] <= best_thresh]\n",
    "    Y_left = Y[X[:, best_feat] <= best_thresh]\n",
    "    X_right = X[X[:, best_feat] > best_thresh]\n",
    "    Y_right = Y[X[:, best_feat] > best_thresh]\n",
    "    tree[node_pos] = [best_feat, best_thresh, 0]\n",
    "    split_node_dec(tree, 2*node_pos+1, X_left, Y_left, depth+1, num_nodes_stop, criterion, stop_depth)\n",
    "    split_node_dec(tree, 2*node_pos+2, X_right, Y_right, depth+1, num_nodes_stop, criterion, stop_depth)\n",
    "    return\n",
    "\n",
    "def split_dec(criterion, pos_l, left, pos_r, right):\n",
    "    if criterion == 'accuracy':\n",
    "        val = (pos_l + right - pos_r)/(left+right)\n",
    "        return max(val, 1-val)\n",
    "    elif criterion == 'entropy':\n",
    "        p_l = left/(left+right)\n",
    "        h_l = h_func(pos_l/(left+1e-40))\n",
    "        p_r = 1 - p_l\n",
    "        h_r = h_func(pos_r/(right+1e-40))\n",
    "        return (p_l*h_l + p_r*h_r)\n",
    "\n",
    "def h_func(x):\n",
    "    if x == 0 or x == 1:\n",
    "        return 0\n",
    "    return x*np.log2(x) + (1-x)*np.log2(1-x)\n",
    "\n",
    "def pred_class(tree, node_pos, sample):\n",
    "    if tree[node_pos][0] == -1:\n",
    "        return tree[node_pos][2]\n",
    "    loc = tree[node_pos][0]\n",
    "    thresh = tree[node_pos][1]\n",
    "    if sample[loc] <= thresh:\n",
    "        return pred_class(tree, 2*node_pos+1, sample)\n",
    "    else:\n",
    "        return pred_class(tree, 2*node_pos+2, sample)\n",
    "    \n",
    "    \n",
    "def train_valid_split(X_train, Y_train, split = 0.8):\n",
    "    indices = [i for i in range(len(X_train))]\n",
    "    train_indices = np.random.choice(indices, size=round(len(indices) * split), replace = False)\n",
    "    valid_indices = list(set(indices) - set(train_indices))\n",
    "    X_tr = []\n",
    "    Y_tr = []\n",
    "    X_valid = []\n",
    "    Y_valid = []\n",
    "    for index in train_indices:\n",
    "        X_tr.append(X_train[index])\n",
    "        Y_tr.append(Y_train[index])\n",
    "    for index in valid_indices:\n",
    "        X_valid.append(X_train[index])\n",
    "        Y_valid.append(Y_train[index])\n",
    "    return np.array(X_tr), np.array(Y_tr), np.array(X_valid), np.array(Y_valid)\n",
    "\n",
    "def preprocess(file):\n",
    "    data = np.load(file)\n",
    "    X_train, Y_train, X_test, Y_test = np.array(data['arr_0']), np.array(data['arr_1']), np.array(data['arr_2']), np.array(data['arr_3'])\n",
    "    #Y_train = np.vstack(Y_train)\n",
    "    #Y_test = np.vstack(Y_test)\n",
    "    mean, std = standardize(X_train)\n",
    "    X_train = X_train-mean\n",
    "    X_train = X_train/std\n",
    "    X_test = X_test - mean\n",
    "    X_test = X_test/std\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def standardize(X):\n",
    "    mean = np.mean(X, axis = 0)\n",
    "    std = np.std(X, axis = 0)\n",
    "    return mean, (std + 1e-20)\n",
    "\n",
    "def choose_hyperparam(X_train, Y_train, X_test, Y_test, criterion):\n",
    "    node_stop = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "    err = 1\n",
    "    best_num_nodes_stop = 0\n",
    "    for ns in node_stop:\n",
    "        clf = train_decision_tree(X_train, Y_train, ns, criterion)\n",
    "        Y_pred = eval_decision_tree(clf, X_test)\n",
    "        test_err = np.sum(Y_pred != Y_test) / len(Y_pred)\n",
    "        if test_err < err:\n",
    "            err = test_err\n",
    "            best_num_nodes_stop = ns\n",
    "    return best_num_nodes_stop\n",
    "\n",
    "def give_result(file, criteria):\n",
    "    X_train, Y_train, X_test, Y_test = preprocess(file)\n",
    "    # print(X_train.shape)\n",
    "    # print(Y_train.shape)\n",
    "    # print(X_test.shape)\n",
    "    # print(Y_test.shape)\n",
    "    X_tr, Y_tr, X_val, Y_val = train_valid_split(X_train, Y_train)\n",
    "    results = []\n",
    "    char1 = file[-5]\n",
    "    for criterion in criteria:\n",
    "        num_nodes_stop = choose_hyperparam(X_tr, Y_tr, X_val, Y_val, criterion)\n",
    "        clf =  train_decision_tree(X_train, Y_train, num_nodes_stop, criterion)\n",
    "        Y_pred_train = eval_decision_tree(clf, X_train)\n",
    "        #print(Y_pred_train.shape)\n",
    "        Y_pred_test = eval_decision_tree(clf, X_test)\n",
    "        #print(Y_pred_test.shape)\n",
    "        train_err = np.sum(Y_pred_train != Y_train) / len(Y_train)\n",
    "        test_err = np.sum(Y_pred_test != Y_test) / len(Y_test)\n",
    "        dict1 = {'Dataset':char1, 'Criterion': criterion,'num_nodes_stop': num_nodes_stop , 'Train loss': train_err, 'Test Loss': test_err}\n",
    "        results.append(dict1)\n",
    "    df1 = pd.DataFrame(results)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a680c34c",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d7de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(X, Y, num_trees=10, num_nodes_stop=1, \n",
    "                        criterion='accuracy', a=0.5, b=0.5):\n",
    "    \"\"\" Returns a random forest trained on X and Y. \n",
    "    Trains num_trees.\n",
    "    Stops splitting nodes in each tree when a node has hit a size of \"num_nodes_stop\" or lower.\n",
    "    Split criterion can be either 'accuracy' or 'entropy'.\n",
    "    Fraction of data used per tree = a\n",
    "    Fraction of features used in each node = b\n",
    "    Returns a random forest (In whatever format that you find appropriate)\n",
    "    \"\"\"\n",
    "    n,d = X.shape\n",
    "    rf = []\n",
    "    for i in range(num_trees):\n",
    "        frac_data = np.sort(np.random.choice(range(n), int(n*a), replace = False))\n",
    "        rf.append(train_dt_rf(X[frac_data], Y[frac_data], num_nodes_stop, criterion, b))\n",
    "    return rf\n",
    "    \n",
    "\n",
    "def eval_random_forest(random_forest, test_X):\n",
    "    \"\"\" Takes in a  random forest object (hhowever you want to store it), and a bunch of instances X and \n",
    "    returns the tree predicted values at those instances.\"\"\"\n",
    "    m, dim = test_X.shape\n",
    "    Y_pred = np.zeros(m)\n",
    "    for tree in random_forest:\n",
    "        Y_pred = Y_pred + eval_dt_rf(tree, test_X)\n",
    "    return np.sign(Y_pred)\n",
    "\n",
    "def train_dt_rf(X, Y, num_nodes_stop, criterion, b):\n",
    "    tree = {}\n",
    "    init_val = -1\n",
    "    tree[0] = [init_val]*3\n",
    "    split_node_dec_rf(tree, 0, X, Y, 0, b, num_nodes_stop, criterion)\n",
    "    return tree\n",
    "\n",
    "def eval_dt_rf(tree, test_X):\n",
    "    n = test_X.shape[0]\n",
    "    Y = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        Y[i] = pred_class_rf(tree, 0, test_X[i])\n",
    "    return Y\n",
    "\n",
    "\n",
    "def pred_class_rf(tree, node_pos, sample):\n",
    "    if tree[node_pos][0] == -1:\n",
    "        return tree[node_pos][2]\n",
    "    loc = tree[node_pos][0]\n",
    "    thresh = tree[node_pos][1]\n",
    "    if sample[loc] <= thresh:\n",
    "        return pred_class(tree, 2*node_pos+1, sample)\n",
    "    else:\n",
    "        return pred_class(tree, 2*node_pos+2, sample)\n",
    "\n",
    "\n",
    "neg = 0\n",
    "pos = 0\n",
    "\n",
    "def split_node_dec_rf(tree, node_pos, X, Y, depth, b, num_nodes_stop=1, criterion = 'accuracy', stop_depth = 1e7):\n",
    "    global neg, pos\n",
    "    n,d = X.shape\n",
    "    total_neg = np.sum((Y==-1))\n",
    "    total_pos = np.sum((Y==1))\n",
    "    one_class = False\n",
    "    if total_pos == 0 or total_pos == n:\n",
    "        one_class = True\n",
    "    if n <= num_nodes_stop or depth >= stop_depth or ((pos == total_pos) and (neg == total_neg)) or one_class:\n",
    "        if total_pos > total_neg:\n",
    "            class_pred = 1\n",
    "        else:\n",
    "            class_pred = -1\n",
    "        tree[node_pos] = [-1,-1, class_pred]\n",
    "        return\n",
    "\n",
    "    neg = total_neg\n",
    "    pos = total_pos\n",
    "    best_feat = 0\n",
    "    best_thresh = 0\n",
    "    best_acc = -np.inf\n",
    "    feats = np.sort(np.random.choice(range(d), int(d*b), replace = False))\n",
    "    for feat in feats:\n",
    "        ids = X[:,feat].argsort()\n",
    "        X = X[ids]\n",
    "        Y = Y[ids]\n",
    "        min_feat = X[0][feat]\n",
    "        max_feat = X[-1][feat]\n",
    "        ### check here\n",
    "        num_pts  = 11\n",
    "        iter = max_feat - min_feat\n",
    "        iter = iter/num_pts\n",
    "        for pt in range(1, num_pts):\n",
    "            thresh = min_feat + pt*iter\n",
    "            pos_l = 0\n",
    "            left = 0\n",
    "            for sample in range(n):\n",
    "                if X[sample][feat] <= thresh:\n",
    "                    left+=1\n",
    "                    if Y[sample]>0:\n",
    "                        pos_l+=1\n",
    "            pos_r = total_pos - pos_l\n",
    "            right = n - left\n",
    "            dec = split_dec_rf(criterion, pos_l, left, pos_r, right)\n",
    "            if dec >= best_acc:\n",
    "                best_acc = dec\n",
    "                best_feat = feat\n",
    "                best_thresh = thresh\n",
    "    X_left = X[X[:, best_feat] <= best_thresh]\n",
    "    Y_left = Y[X[:, best_feat] <= best_thresh]\n",
    "    X_right = X[X[:, best_feat] > best_thresh]\n",
    "    Y_right = Y[X[:, best_feat] > best_thresh]\n",
    "    tree[node_pos] = [best_feat, best_thresh, 0]\n",
    "    split_node_dec_rf(tree, 2*node_pos+1, X_left, Y_left, depth+1, b, num_nodes_stop, criterion, stop_depth)\n",
    "    split_node_dec_rf(tree, 2*node_pos+2, X_right, Y_right, depth+1, b, num_nodes_stop, criterion, stop_depth)\n",
    "    return\n",
    "\n",
    "def split_dec_rf(criterion, pos_l, left, pos_r, right):\n",
    "    if criterion == 'accuracy':\n",
    "        val = (pos_l + right - pos_r)/(left+right)\n",
    "        return max(val, 1-val)\n",
    "    elif criterion == 'entropy':\n",
    "        p_l = left/(left+right)\n",
    "        h_l = h_func(pos_l/(left+1e-40))\n",
    "        p_r = 1 - p_l\n",
    "        h_r = h_func(pos_r/(right+1e-40))\n",
    "        return (p_l*h_l + p_r*h_r)\n",
    "\n",
    "def h_func(x):\n",
    "    if x == 0 or x == 1:\n",
    "        return 0\n",
    "    return x*np.log2(x) + (1-x)*np.log2(1-x)\n",
    "\n",
    "def train_valid_split(X_train, Y_train, split = 0.8):\n",
    "    indices = [i for i in range(len(X_train))]\n",
    "    train_indices = np.random.choice(indices, size=round(len(indices) * split), replace = False)\n",
    "    valid_indices = list(set(indices) - set(train_indices))\n",
    "    X_tr = []\n",
    "    Y_tr = []\n",
    "    X_valid = []\n",
    "    Y_valid = []\n",
    "    for index in train_indices:\n",
    "        X_tr.append(X_train[index])\n",
    "        Y_tr.append(Y_train[index])\n",
    "    for index in valid_indices:\n",
    "        X_valid.append(X_train[index])\n",
    "        Y_valid.append(Y_train[index])\n",
    "    return np.array(X_tr), np.array(Y_tr), np.array(X_valid), np.array(Y_valid)\n",
    "\n",
    "def preprocess(file):\n",
    "    data = np.load(file)\n",
    "    X_train, Y_train, X_test, Y_test = np.array(data['arr_0']), np.array(data['arr_1']), np.array(data['arr_2']), np.array(data['arr_3'])\n",
    "    #Y_train = np.vstack(Y_train)\n",
    "    #Y_test = np.vstack(Y_test)\n",
    "    mean, std = standardize(X_train)\n",
    "    X_train = X_train-mean\n",
    "    X_train = X_train/std\n",
    "    X_test = X_test - mean\n",
    "    X_test = X_test/std\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def standardize(X):\n",
    "    mean = np.mean(X, axis = 0)\n",
    "    std = np.std(X, axis = 0)\n",
    "    return mean, (std + 1e-20)\n",
    "\n",
    "def choose_hyperparam(X_train, Y_train, X_test, Y_test, criterion):\n",
    "    num_trees = [5, 10, 20, 30, 40, 50, 60, 70, 80]\n",
    "    err = 1\n",
    "    best_num_trees = 0\n",
    "    for nt in num_trees:\n",
    "        clf = train_random_forest(X_train, Y_train, nt, 1, criterion)\n",
    "        Y_pred = eval_random_forest(clf, X_test)\n",
    "        test_err = np.sum(Y_pred != Y_test) / len(Y_pred)\n",
    "        if test_err < err:\n",
    "            err = test_err\n",
    "            best_num_trees = nt\n",
    "    return best_num_trees\n",
    "\n",
    "def give_result(file, criteria):\n",
    "    X_train, Y_train, X_test, Y_test = preprocess(file)\n",
    "    # print(X_train.shape)\n",
    "    # print(Y_train.shape)\n",
    "    # print(X_test.shape)\n",
    "    # print(Y_test.shape)\n",
    "    X_tr, Y_tr, X_val, Y_val = train_valid_split(X_train, Y_train)\n",
    "    results = []\n",
    "    char1 = file[-5]\n",
    "    for criterion in criteria:\n",
    "        num_trees = choose_hyperparam(X_tr, Y_tr, X_val, Y_val, criterion)\n",
    "        clf =  train_random_forest(X_train, Y_train, num_trees, 1, criterion)\n",
    "        Y_pred_train = eval_random_forest(clf, X_train)\n",
    "        #print(Y_pred_train.shape)\n",
    "        Y_pred_test = eval_random_forest(clf, X_test)\n",
    "        #print(Y_pred_test.shape)\n",
    "        train_err = np.sum(Y_pred_train != Y_train) / len(Y_train)\n",
    "        test_err = np.sum(Y_pred_test != Y_test) / len(Y_test)\n",
    "        dict1 = {'Dataset':char1, 'Criterion': criterion,'Number of trees': num_trees , 'Train loss': train_err, 'Test Loss': test_err}\n",
    "        results.append(dict1)\n",
    "    df1 = pd.DataFrame(results)\n",
    "    return df1\n",
    "\n",
    "criteria = ['accuracy', 'entropy']\n",
    "file_name = '/content/Data/dataset_D.npz'\n",
    "## uncomment the following two blocks to see the most recurring hyperparams over 10 iterations\n",
    "# i = 10\n",
    "# while i > 0:\n",
    "#     result = give_result(file_name, criteria)\n",
    "#     print(result)\n",
    "#     i-=1\n",
    "## uncomment the following block of code for getting one iteration of results    \n",
    "# result = give_result(file_name, criteria)\n",
    "# result\n",
    "X_train, Y_train, X_test, Y_test = preprocess('/content/Data/dataset_A.npz')\n",
    "X_min = min(X_train[:,0]),min(X_train[:,1])\n",
    "X_max = max(X_train[:,0]),max(X_train[:,1])\n",
    "X,Y = np.meshgrid(np.arange(X_min[0]-0.5,X_max[0]+0.5,0.05),np.arange(X_min[1]-0.5,X_max[1]+0.5,0.05))\n",
    "testing_set= np.concatenate([X.reshape(-1,1),Y.reshape(-1,1)],axis=1)\n",
    "clf = train_random_forest(X_train, Y_train, num_trees = 30, num_nodes_stop = 1, criterion='accuracy')\n",
    "predA = eval_random_forest(clf, testing_set)\n",
    "\n",
    "plt.figure(0)\n",
    "f, ax = plt.subplots(1,1,sharex=False,sharey=True,figsize=(12,8))\n",
    "f.suptitle('Random Forest Classification on dataset A', size=18)\n",
    "ax.contourf(X,Y,predA.reshape(X.shape), cmap = mpl.colors.ListedColormap(('skyblue', 'gold')), alpha=0.3)\n",
    "ax.scatter(X_train[Y_train==1][:,0], X_train[Y_train==1][:,1], s=2,c='darkorange', label='Class 1')\n",
    "ax.scatter(X_train[Y_train==-1][:,0], X_train[Y_train==-1][:,1], s=2, c='blue',label='Class -1')\n",
    "ax.set_title(f'Plot when criterion is accuracy')\n",
    "ax.set_xlabel(r'$x_{1}\\rightarrow$',size=15)\n",
    "ax.set_ylabel(r'$x_{2}\\rightarrow$',size=15)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = preprocess('/content/Data/dataset_B.npz')\n",
    "X_min = min(X_train[:,0]),min(X_train[:,1])\n",
    "X_max = max(X_train[:,0]),max(X_train[:,1])\n",
    "X,Y = np.meshgrid(np.arange(X_min[0]-0.5,X_max[0]+0.5,0.05),np.arange(X_min[1]-0.5,X_max[1]+0.5,0.05))\n",
    "testing_set= np.concatenate([X.reshape(-1,1),Y.reshape(-1,1)],axis=1)\n",
    "clf = train_random_forest(X_train, Y_train, num_trees = 50, num_nodes_stop = 1, criterion='accuracy')\n",
    "predB = eval_random_forest(clf, testing_set)\n",
    "\n",
    "plt.figure(0)\n",
    "f, ax = plt.subplots(1,1,sharex=False,sharey=True,figsize=(12,8))\n",
    "f.suptitle('Random Forest Classification on dataset B', size=18)\n",
    "ax.contourf(X,Y,predB.reshape(X.shape), cmap = mpl.colors.ListedColormap(('skyblue', 'gold')), alpha=0.3)\n",
    "ax.scatter(X_train[Y_train==1][:,0], X_train[Y_train==1][:,1], s=2,c='darkorange', label='Class 1')\n",
    "ax.scatter(X_train[Y_train==-1][:,0], X_train[Y_train==-1][:,1], s=2, c='blue',label='Class -1')\n",
    "ax.set_title(f'Plot when criterion is accuracy')\n",
    "ax.set_xlabel(r'$x_{1}\\rightarrow$',size=15)\n",
    "ax.set_ylabel(r'$x_{2}\\rightarrow$',size=15)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
