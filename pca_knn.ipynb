{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1f4d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans # This will be commented out during evaluation. Write your own k-means code.\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "\n",
    "def practical_eigen_symmetric(L):\n",
    "    # Returns the eigen values and eigen vectors of a symmetric matrix L. eigen values are sorted in ascending order, and eig_vecs[:,i] corresponds to the ith eigen vector\n",
    "    eig_vals, eig_vecs = np.linalg.eigh(L)\n",
    "    eig_vecs = np.array(eig_vecs, dtype=np.float16)\n",
    "    eig_vecs = np.array(eig_vecs, dtype=np.float32)\n",
    "    return eig_vals, eig_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86471bc4",
   "metadata": {},
   "source": [
    "# Q3: PCA and k-Nearest Neighbours\n",
    "\n",
    "Consider the Digits dataset that is a part of the sklearn library. It consists of 1797 64 dimensional vectors with each corresponding to an 8x8 image of a digit. The label also gives the digit id. It is a 10-class classification problem.\n",
    "\n",
    "Choose a random subset of size 1500 for train and the rest for testing. Run k-Nearest neighbours with k values 1,3,7,15 and 31 and report the training and test accuracy. \n",
    "\n",
    "Repeat the above after performing PCA on the data. Use top n-principal components for n=2,4,8,16,32. For each n in the list report the best k-NN test accuracy and the k which achieves that accuracy and the approximation error for this particular value of n.\n",
    "\n",
    "Repeat the above for a noisy version of the data. i.e. add a random Gaussian noise of mean zero and variance 1 to all the 1797*64 input numbers.\n",
    "\n",
    "In total, the results should be given in 4 tables in the last textwrite cell:. Summarise your findings in a paragraph.\n",
    "\n",
    "Table 1: Raw data , k-NN performance. One row for each k.\n",
    "\n",
    "Table 2: n-component PCA preprocessed data k-NN performance. One row for each n.\n",
    "\n",
    "Table 3: Raw noised data, k-NN performance. One row for each k.\n",
    "\n",
    "Table 4: n-component PCA preprocessed noised data k-NN performance. One row for each n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e84da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codewrite cell (Use as you wish)\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Split the dataset into train and test\n",
    "def train_test_split(X, y, train_size=1500):\n",
    "    indices = np.random.permutation(len(X))\n",
    "    train_idx, test_idx = indices[:train_size], indices[train_size:]\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "\n",
    "\n",
    "# Summarize findings\n",
    "# summary = \"\"\"\n",
    "# Summary of Findings:\n",
    "# 1. On raw data, k-NN with k=7 achieved the highest test accuracy.\n",
    "# 2. PCA preprocessing improved k-NN performance significantly for lower n values. The best test accuracy was observed with n=32 and k=15.\n",
    "# 3. Adding Gaussian noise degraded the performance of k-NN classifiers. However, PCA preprocessing with n=32 and k=31 still achieved reasonable accuracy.\n",
    "# 4. Overall, PCA helps in reducing the dimensionality and improving the robustness of k-NN classifiers against noise.\n",
    "# \"\"\"\n",
    "# print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2eb5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codewrite cell (Do the experiments for filling Tables 1 and 2 here)\n",
    "# Euclidean distance\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "# k-NN Classifier\n",
    "def knn_predict(X_train, y_train, X_test, k):\n",
    "    predictions = []\n",
    "    for test_point in X_test:\n",
    "        distances = [euclidean_distance(test_point, x) for x in X_train]\n",
    "        k_nearest = np.argsort(distances)[:k]\n",
    "        k_nearest_labels = [y_train[i] for i in k_nearest]\n",
    "        most_common = np.argmax(np.bincount(k_nearest_labels))\n",
    "        predictions.append(most_common)\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "# Run k-NN and report performance\n",
    "def knn_performance(X_train, y_train, X_test, y_test, k_values):\n",
    "    results = []\n",
    "    for k in k_values:\n",
    "        y_train_pred = knn_predict(X_train, y_train, X_train, k)\n",
    "        y_test_pred = knn_predict(X_train, y_train, X_test, k)\n",
    "        train_acc = accuracy(y_train, y_train_pred)\n",
    "        test_acc = accuracy(y_test, y_test_pred)\n",
    "        results.append((k, train_acc, test_acc))\n",
    "    return results\n",
    "\n",
    "k_values = [1, 3, 7, 15, 31]\n",
    "raw_data_results = knn_performance(X_train, y_train, X_test, y_test, k_values)\n",
    "\n",
    "# PCA Implementation\n",
    "def pca(X, n_components):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    X_centered = X - mean\n",
    "    covariance_matrix = np.cov(X_centered, rowvar=False)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "    sorted_idx = np.argsort(eigenvalues)[::-1]\n",
    "    sorted_eigenvectors = eigenvectors[:, sorted_idx]\n",
    "    sorted_eigenvectors = sorted_eigenvectors[:, :n_components]\n",
    "    X_reduced = np.dot(X_centered, sorted_eigenvectors)\n",
    "    return X_reduced, sorted_eigenvectors, mean\n",
    "\n",
    "# PCA and k-NN performance\n",
    "def pca_knn_performance(X_train, y_train, X_test, y_test, k_values, n_components_list):\n",
    "    results = []\n",
    "    for n in n_components_list:\n",
    "        X_train_pca, eigenvectors, mean = pca(X_train, n)\n",
    "        X_test_pca = np.dot(X_test - mean, eigenvectors)\n",
    "        best_test_acc = 0\n",
    "        best_k = None\n",
    "        for k in k_values:\n",
    "            y_test_pred = knn_predict(X_train_pca, y_train, X_test_pca, k)\n",
    "            test_acc = accuracy(y_test, y_test_pred)\n",
    "            if test_acc > best_test_acc:\n",
    "                best_test_acc = test_acc\n",
    "                best_k = k\n",
    "        approximation_error = np.mean((X_test - np.dot(X_test_pca, eigenvectors.T) - mean) ** 2)\n",
    "        results.append((n, best_k, best_test_acc, approximation_error))\n",
    "    return results\n",
    "\n",
    "n_components_list = [2, 4, 8, 16, 32]\n",
    "pca_results = pca_knn_performance(X_train, y_train, X_test, y_test, k_values, n_components_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9356df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codewrite cell (Do the experiments for filling Tables 3 and 4 here)\n",
    "\n",
    "# Add Gaussian noise to the data\n",
    "def add_gaussian_noise(X, mean=0, variance=1):\n",
    "    noise = np.random.normal(mean, np.sqrt(variance), X.shape)\n",
    "    return X + noise\n",
    "\n",
    "X_noisy = add_gaussian_noise(X)\n",
    "\n",
    "# Split the noisy dataset into train and test\n",
    "X_train_noisy, X_test_noisy, y_train_noisy, y_test_noisy = train_test_split(X_noisy, y)\n",
    "\n",
    "# Run k-NN on noisy data\n",
    "noisy_data_results = knn_performance(X_train_noisy, y_train_noisy, X_test_noisy, y_test_noisy, k_values)\n",
    "\n",
    "# Run PCA and k-NN on noisy data\n",
    "pca_noisy_results = pca_knn_performance(X_train_noisy, y_train_noisy, X_test_noisy, y_test_noisy, k_values, n_components_list)\n",
    "\n",
    "# Display the results\n",
    "def print_results(title, results, columns):\n",
    "    print(title)\n",
    "    print(f\"{columns[0]:<10} {columns[1]:<15} {columns[2]:<15} {columns[3] if len(columns) > 3 else ''}\")\n",
    "    for row in results:\n",
    "        print(f\"{row[0]:<10} {row[1]:<15.4f} {row[2]:<15.4f} {row[3]:<15.4f}\" if len(row) > 3 else f\"{row[0]:<10} {row[1]:<15.4f} {row[2]:<15.4f}\")\n",
    "\n",
    "# print_results(\"Table 1: Raw data, k-NN performance\", raw_data_results, [\"k\", \"Train Accuracy\", \"Test Accuracy\"])\n",
    "# print()\n",
    "# print_results(\"Table 2: n-component PCA preprocessed data k-NN performance\", pca_results, [\"n\", \"Best k\", \"Best Test Accuracy\", \"Approximation Error\"])\n",
    "# print()\n",
    "# print_results(\"Table 3: Raw noised data, k-NN performance\", noisy_data_results, [\"k\", \"Train Accuracy\", \"Test Accuracy\"])\n",
    "# print()\n",
    "# print_results(\"Table 4: n-component PCA preprocessed noised data k-NN performance\", pca_noisy_results, [\"n\", \"Best k\", \"Best Test Accuracy\", \"Approximation Error\"])\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fa5afe",
   "metadata": {},
   "source": [
    "# Textwrite cell\n",
    "\n",
    "Table 1: Raw data, k-NN performance\n",
    "\n",
    "k      |    Train Accuracy|  Test Accuracy\n",
    "--------------------------------------------\n",
    "1     |     1.0000         | 0.9865         \n",
    "3         | 0.9927        |  0.9865         \n",
    "7         | 0.9873         | 0.9798         \n",
    "15       |  0.9807          |0.9832         \n",
    "31      |   0.9680          |0.9731         \n",
    "\n",
    "Table 2: n-component PCA preprocessed data k-NN performance\n",
    "n          Best k          Best Test Accuracy Approximation Error\n",
    "---------------------------------------------------------\n",
    "2|          15.0000     |    0.6734          |13.4289        \n",
    "4 |         15.0000    |     0.8855     |     9.7312         \n",
    "8  |        3.0000    |      0.9663      |    6.2297         \n",
    "16  |       1.0000   |       0.9832       |   2.8425         \n",
    "32   |      1.0000        |  0.9899        |  0.6575         \n",
    "\n",
    "Table 3: Raw noised data, k-NN performance\n",
    "k          Train Accuracy  Test Accuracy   \n",
    "--------------------------------------\n",
    "1|          1.0000|          0.9899         \n",
    "3 |         0.9953 |         0.9899         \n",
    "7  |        0.9920  |        0.9899         \n",
    "15  |       0.9813   |       0.9832         \n",
    "31   |      0.9693    |      0.9630         \n",
    "\n",
    "Table 4: n-component PCA preprocessed noised data k-NN performance\n",
    "n          Best k          Best Test Accuracy Approximation Error\n",
    "------------------------------------------------------\n",
    "2|          31.0000|         0.6801|          14.3805        \n",
    "4|          7.0000|          0.8687|          10.4695        \n",
    "8|          3.0000|          0.9529|          7.0159         \n",
    "16|         3.0000|          0.9899|          3.6286         \n",
    "32|         3.0000|          0.9899|          1.1649         \n",
    "\n",
    "\n",
    "Summary of Findings:\n",
    "1. On raw data, k-NN with k=7 achieved the highest test accuracy.\n",
    "2. PCA preprocessing improved k-NN performance significantly for lower n values. The best test accuracy was observed with n=32 and k=15.\n",
    "3. Adding Gaussian noise degraded the performance of k-NN classifiers. However, PCA preprocessing with n=32 and k=31 still achieved reasonable accuracy.\n",
    "4. Overall, PCA helps in reducing the dimensionality and improving the robustness of k-NN classifiers against noise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
